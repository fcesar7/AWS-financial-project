{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "from fredapi import Fred\n",
    "from alpha_vantage.timeseries import TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = os.getcwd()\n",
    "path = os.path.dirname(current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interest Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is no way of specifying start date for this security with this API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_us_rates(path,key_AV):\n",
    "\n",
    "    function = 'FEDERAL_FUNDS_RATE'   # Effective Federal Funds Rate\n",
    "    url = f'https://www.alphavantage.co/query?function={function}&apikey={key_AV}'\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    interest_rates = pd.DataFrame(data)\n",
    "    interest_rates = interest_rates['data'].apply(lambda x: pd.Series(x))\n",
    "    interest_rates.rename(columns={'value':'us_rates_%'},inplace=True)\n",
    "    interest_rates['date'] = pd.to_datetime(interest_rates['date'],format='%Y-%m-%d')\n",
    "    interest_rates['us_rates_%'] = interest_rates['us_rates_%'].astype(float)\n",
    "    interest_rates = interest_rates.sort_values('date').reset_index(drop=True)\n",
    "    interest_rates = interest_rates[interest_rates['date']>='2000-01-01']\n",
    "\n",
    "    # create date range from 2000 until today and fill gaps\n",
    "    start_date = '2000-01-01'\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d') \n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "    us_rates = date_range.merge(interest_rates,how='outer',on='date')\n",
    "    us_rates = us_rates.ffill()\n",
    "    us_rates.to_csv(path+\"/data_staging/us_rates.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S&P 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snp(path,key_AV):\n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    symbol = 'SPY'  # S&P 500 ETF\n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/snp.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10] # extract since last 10th available date (possible updates in recent data)\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}'\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}&outputsize=full'\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    sp500 = pd.DataFrame(data)\n",
    "    sp500 = pd.concat([sp500['Meta Data'],sp500['Time Series (Daily)'].apply(lambda x: pd.Series(x))],axis=1)\n",
    "\n",
    "    sp500 = sp500.reset_index()\n",
    "    sp500 = sp500.iloc[5:]\n",
    "    sp500.drop(columns=['Meta Data',0],inplace=True)\n",
    "    sp500.rename(columns={'index':'date'},inplace=True)\n",
    "    sp500.columns = [f'sp500{col[2:]}' if i >= 1 else col for i, col in enumerate(sp500.columns)]\n",
    "\n",
    "    sp500['date'] = pd.to_datetime(sp500['date'],format='%Y-%m-%d')\n",
    "    sp500['sp500 open'] = pd.to_numeric(sp500['sp500 open'], errors='coerce')\n",
    "    sp500['sp500 high'] = pd.to_numeric(sp500['sp500 high'], errors='coerce')\n",
    "    sp500['sp500 low'] = pd.to_numeric(sp500['sp500 low'], errors='coerce')\n",
    "    sp500['sp500 close'] = pd.to_numeric(sp500['sp500 close'], errors='coerce')\n",
    "    sp500['sp500 volume'] = pd.to_numeric(sp500['sp500 volume'], errors='coerce')\n",
    "    sp500['sp500 high-low'] = sp500['sp500 high'] - sp500['sp500 low']\n",
    "    sp500 = sp500.sort_values('date')\n",
    "\n",
    "    # create date range from 2000 until today and fill gaps\n",
    "\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d') \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    sp500 = date_range.merge(sp500,how='outer',on='date') # get date range from 2000 until today and fill gaps\n",
    "    sp500 = sp500.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    sp500['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "\n",
    "    snp = pd.concat([sp500,hist])\n",
    "\n",
    "\n",
    "    snp[snp.duplicated(subset=['date'],keep=False)] = snp[(snp.duplicated(subset=['date'],keep=False))&(snp['type']=='new')]\n",
    "    snp = snp[snp['date'].isna()==False]\n",
    "    snp.drop(columns='type',inplace=True)\n",
    "\n",
    "    snp = snp[snp['date']>='2000-01-01']\n",
    "    snp = snp.sort_values('date')\n",
    "    snp = snp.reset_index(drop=True)\n",
    "    snp.to_csv(path+\"/data_staging/snp.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASDAQ 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nasdaq(path,key_AV):\n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    symbol = 'QQQ'  # NASDAQ 100 ETF\n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/nasdaq.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}'\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}&outputsize=full'\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    nsdq = pd.DataFrame(data)\n",
    "    nsdq = pd.concat([nsdq['Meta Data'],nsdq['Time Series (Daily)'].apply(lambda x: pd.Series(x))],axis=1)\n",
    "\n",
    "    nsdq = nsdq.reset_index()\n",
    "    nsdq = nsdq.iloc[5:]\n",
    "    nsdq.drop(columns=['Meta Data',0],inplace=True)\n",
    "    nsdq.rename(columns={'index':'date'},inplace=True)\n",
    "    nsdq.columns = [f'nasdaq{col[2:]}' if i >= 1 else col for i, col in enumerate(nsdq.columns)]\n",
    "\n",
    "    nsdq['date'] = pd.to_datetime(nsdq['date'],format='%Y-%m-%d')\n",
    "    nsdq['nasdaq open'] = pd.to_numeric(nsdq['nasdaq open'], errors='coerce')\n",
    "    nsdq['nasdaq high'] = pd.to_numeric(nsdq['nasdaq high'], errors='coerce')\n",
    "    nsdq['nasdaq low'] = pd.to_numeric(nsdq['nasdaq low'], errors='coerce')\n",
    "    nsdq['nasdaq close'] = pd.to_numeric(nsdq['nasdaq close'], errors='coerce')\n",
    "    nsdq['nasdaq volume'] = pd.to_numeric(nsdq['nasdaq volume'], errors='coerce')\n",
    "    nsdq['nasdaq high-low'] = nsdq['nasdaq high'] - nsdq['nasdaq low']\n",
    "    nsdq = nsdq.sort_values('date')\n",
    "\n",
    "    # create date range from 2000 until today and fill gaps\n",
    "\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d') \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    nsdq = date_range.merge(nsdq,how='outer',on='date') # get date range from 2000 until today and fill gaps\n",
    "    nsdq = nsdq.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    nsdq['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    nasdaq = pd.concat([nsdq,hist])\n",
    "\n",
    "    nasdaq[nasdaq.duplicated(subset=['date'],keep=False)] = nasdaq[(nasdaq.duplicated(subset=['date'],keep=False))&(nasdaq['type']=='new')]\n",
    "    nasdaq = nasdaq[nasdaq['date'].isna()==False]\n",
    "    nasdaq.drop(columns='type',inplace=True)\n",
    "\n",
    "    nasdaq = nasdaq[nasdaq['date']>='2000-01-01']\n",
    "    nasdaq = nasdaq.sort_values('date')\n",
    "    nasdaq = nasdaq.reset_index(drop=True)\n",
    "    nasdaq.to_csv(path+\"/data_staging/nasdaq.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Price Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cpi(path,key_FRED):\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/cpi.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-80]\n",
    "        start_date = start_date.strftime('%Y-%m-%d')\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()    \n",
    "    end_date = datetime.now().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    key = key_FRED\n",
    "    fred = Fred(api_key=key)\n",
    "    series_id = 'CPIAUCSL'\n",
    "\n",
    "    cpi_data = fred.get_series(series_id, observation_start=start_date, observation_end=end_date)\n",
    "    cpi_data = pd.DataFrame(cpi_data).reset_index()\n",
    "    cpi_data.rename(columns={'index':'date',0:'CPI'},inplace=True)\n",
    "    cpi_data = cpi_data[cpi_data['date']>='2000-01-01']\n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    cpi_data = date_range.merge(cpi_data,how='outer',on='date')\n",
    "    cpi_data = cpi_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    cpi_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    cpi = pd.concat([cpi_data,hist])\n",
    "\n",
    "    cpi[cpi.duplicated(subset=['date'],keep=False)] = cpi[(cpi.duplicated(subset=['date'],keep=False))&(cpi['type']=='new')]\n",
    "    cpi = cpi[cpi['date'].isna()==False]\n",
    "    cpi.drop(columns='type',inplace=True)\n",
    "\n",
    "    cpi = cpi.sort_values(by='date')\n",
    "    cpi = cpi.reset_index(drop=True)\n",
    "    cpi.to_csv(path+\"/data_staging/cpi.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USD / CHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_usd_chf(path, key_FRED):\n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/usd_chf.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        start_date = start_date.strftime('%Y-%m-%d')\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()    \n",
    "        \n",
    "    end_date = datetime.now().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    key = key_FRED\n",
    "    fred = Fred(api_key=key)\n",
    "    series_id = 'DEXSZUS'\n",
    "\n",
    "    usd_chf_data = fred.get_series(series_id, observation_start=start_date, observation_end=end_date)\n",
    "    usd_chf_data = pd.DataFrame(usd_chf_data).reset_index()\n",
    "    usd_chf_data.rename(columns={'index':'date',0:'usd_chf'},inplace=True)\n",
    "    usd_chf_data = usd_chf_data[usd_chf_data['date']>='2000-01-01']\n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    usd_chf_data = date_range.merge(usd_chf_data,how='outer',on='date')\n",
    "    usd_chf_data = usd_chf_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    usd_chf_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    usd_chf = pd.concat([usd_chf_data,hist])\n",
    "    usd_chf[usd_chf.duplicated(subset=['date'],keep=False)] = usd_chf[(usd_chf.duplicated(subset=['date'],keep=False))&(usd_chf['type']=='new')]\n",
    "\n",
    "    usd_chf = usd_chf[usd_chf['date'].isna()==False]\n",
    "    usd_chf.drop(columns='type',inplace=True)\n",
    "\n",
    "    usd_chf = usd_chf.sort_values(by='date')\n",
    "    usd_chf = usd_chf.reset_index(drop=True)\n",
    "    usd_chf.to_csv(path+\"/data_staging/usd_chf.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUR / USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eur_usd(path, key_FRED):\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/eur_usd.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        start_date = start_date.strftime('%Y-%m-%d')\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()    \n",
    "        \n",
    "    end_date = datetime.now().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    key = key_FRED\n",
    "    fred = Fred(api_key=key)\n",
    "    series_id = 'DEXUSEU'\n",
    "\n",
    "    eur_usd_data = fred.get_series(series_id, observation_start=start_date, observation_end=end_date)\n",
    "    eur_usd_data = pd.DataFrame(eur_usd_data).reset_index()\n",
    "    eur_usd_data.rename(columns={'index':'date',0:'eur_usd'},inplace=True)\n",
    "    eur_usd_data = eur_usd_data[eur_usd_data['date']>='2000-01-01']\n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    eur_usd_data = date_range.merge(eur_usd_data,how='outer',on='date')\n",
    "    eur_usd_data = eur_usd_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    eur_usd_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    eur_usd = pd.concat([eur_usd_data,hist])\n",
    "    eur_usd[eur_usd.duplicated(subset=['date'],keep=False)] = eur_usd[(eur_usd.duplicated(subset=['date'],keep=False))&(eur_usd['type']=='new')]\n",
    "\n",
    "    eur_usd = eur_usd[eur_usd['date'].isna()==False]\n",
    "    eur_usd.drop(columns='type',inplace=True)\n",
    "\n",
    "    eur_usd = eur_usd.sort_values(by='date')\n",
    "    eur_usd = eur_usd.reset_index(drop=True)\n",
    "    eur_usd.to_csv(path+\"/data_staging/eur_usd.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gdp(path,key_FRED):\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/gdp.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-110]\n",
    "        start_date = start_date.strftime('%Y-%m-%d')\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()    \n",
    "    end_date = datetime.now().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    key = key_FRED\n",
    "    fred = Fred(api_key=key)\n",
    "    series_id = 'GDP' # Gross Domestic Product\n",
    "    gdp_data = fred.get_series(series_id, observation_start=start_date, observation_end=end_date)\n",
    "    gdp_data = pd.DataFrame(gdp_data).reset_index()\n",
    "    gdp_data.rename(columns={'index':'date',0:'GDP'},inplace=True)\n",
    "    gdp_data = gdp_data[gdp_data['date']>='2000-01-01']\n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    gdp_data = date_range.merge(gdp_data,how='outer',on='date')\n",
    "    gdp_data = gdp_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    gdp_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    gdp = pd.concat([gdp_data,hist])\n",
    "\n",
    "    gdp[gdp.duplicated(subset=['date'],keep=False)] = gdp[(gdp.duplicated(subset=['date'],keep=False))&(gdp['type']=='new')]\n",
    "    gdp = gdp[gdp['date'].isna()==False]\n",
    "    gdp.drop(columns='type',inplace=True)\n",
    "\n",
    "    gdp = gdp.sort_values(by='date')\n",
    "    gdp = gdp.reset_index(drop=True)\n",
    "    gdp.to_csv(path+\"/data_staging/gdp.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver(path,key_AV):\n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    symbol = 'SIVR'  # silver \n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/silver.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}'\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}&outputsize=full'\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    silver_data = pd.DataFrame(data)\n",
    "    silver_data = pd.concat([silver_data['Meta Data'],silver_data['Time Series (Daily)'].apply(lambda x: pd.Series(x))],axis=1)\n",
    "\n",
    "    silver_data = silver_data.reset_index()\n",
    "    silver_data = silver_data.iloc[5:]\n",
    "    silver_data.drop(columns=['Meta Data',0],inplace=True)\n",
    "    silver_data.rename(columns={'index':'date'},inplace=True)\n",
    "    silver_data.columns = [f'silver{col[2:]}' if i >= 1 else col for i, col in enumerate(silver_data.columns)]\n",
    "\n",
    "    silver_data['date'] = pd.to_datetime(silver_data['date'],format='%Y-%m-%d')\n",
    "    silver_data['silver open'] = pd.to_numeric(silver_data['silver open'], errors='coerce')\n",
    "    silver_data['silver high'] = pd.to_numeric(silver_data['silver high'], errors='coerce')\n",
    "    silver_data['silver low'] = pd.to_numeric(silver_data['silver low'], errors='coerce')\n",
    "    silver_data['silver close'] = pd.to_numeric(silver_data['silver close'], errors='coerce')\n",
    "    silver_data['silver volume'] = pd.to_numeric(silver_data['silver volume'], errors='coerce')\n",
    "    silver_data['silver high-low'] = silver_data['silver high'] - silver_data['silver low']\n",
    "    silver_data = silver_data.sort_values('date')\n",
    "\n",
    "    # create date range from 2000 until today and fill gaps\n",
    "\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d') \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    silver_data = date_range.merge(silver_data,how='outer',on='date') # get date range from 2000 until today and fill gaps\n",
    "    silver_data = silver_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    silver_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    silver = pd.concat([silver_data,hist])\n",
    "\n",
    "    silver[silver.duplicated(subset=['date'],keep=False)] = silver[(silver.duplicated(subset=['date'],keep=False))&(silver['type']=='new')]\n",
    "    silver = silver[silver['date'].isna()==False]\n",
    "    silver.drop(columns='type',inplace=True)\n",
    "\n",
    "    silver = silver[silver['date']>='2000-01-01']\n",
    "    silver = silver[silver['silver open'].isna()==False]\n",
    "    silver = silver.sort_values('date')\n",
    "    silver = silver.reset_index(drop=True)\n",
    "    silver.to_csv(path+\"/data_staging/silver.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oil(path,key_AV):    \n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    symbol = 'USO'  # Oil ETF\n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/oil.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}'\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}&outputsize=full'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    oil_data = pd.DataFrame(data)\n",
    "    oil_data = pd.concat([oil_data['Meta Data'],oil_data['Time Series (Daily)'].apply(lambda x: pd.Series(x))],axis=1)\n",
    "\n",
    "    oil_data = oil_data.reset_index()\n",
    "    oil_data = oil_data.iloc[5:]\n",
    "    oil_data.drop(columns=['Meta Data',0],inplace=True)\n",
    "    oil_data.rename(columns={'index':'date'},inplace=True)\n",
    "    oil_data.columns = [f'oil{col[2:]}' if i >= 1 else col for i, col in enumerate(oil_data.columns)]\n",
    "\n",
    "    oil_data['date'] = pd.to_datetime(oil_data['date'],format='%Y-%m-%d')\n",
    "    oil_data['oil open'] = pd.to_numeric(oil_data['oil open'], errors='coerce')\n",
    "    oil_data['oil high'] = pd.to_numeric(oil_data['oil high'], errors='coerce')\n",
    "    oil_data['oil low'] = pd.to_numeric(oil_data['oil low'], errors='coerce')\n",
    "    oil_data['oil close'] = pd.to_numeric(oil_data['oil close'], errors='coerce')\n",
    "    oil_data['oil volume'] = pd.to_numeric(oil_data['oil volume'], errors='coerce')\n",
    "    oil_data['oil high-low'] = oil_data['oil high'] - oil_data['oil low']\n",
    "    oil_data = oil_data.sort_values('date')\n",
    "\n",
    "    # create date range from 2000 until today and fill gaps\n",
    "\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d') \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    oil_data = date_range.merge(oil_data,how='outer',on='date') # get date range from 2000 until today and fill gaps\n",
    "    oil_data = oil_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    oil_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    oil = pd.concat([oil_data,hist])\n",
    "\n",
    "    oil[oil.duplicated(subset=['date'],keep=False)] = oil[(oil.duplicated(subset=['date'],keep=False))&(oil['type']=='new')]\n",
    "    oil = oil[oil['date'].isna()==False]\n",
    "    oil.drop(columns='type',inplace=True)\n",
    "\n",
    "    oil = oil[oil['date']>='2000-01-01']\n",
    "    oil = oil[oil['oil open'].isna()==False]\n",
    "    oil = oil.sort_values('date')\n",
    "    oil = oil.reset_index(drop=True)\n",
    "    oil.to_csv(path+\"/data_staging/oil.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platinum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_platinum(path,key_AV):\n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    symbol = 'PPLT'  # platinum \n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/platinum.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}'\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}&outputsize=full'\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    platinum_data = pd.DataFrame(data)\n",
    "    platinum_data = pd.concat([platinum_data['Meta Data'],platinum_data['Time Series (Daily)'].apply(lambda x: pd.Series(x))],axis=1)\n",
    "\n",
    "    platinum_data = platinum_data.reset_index()\n",
    "    platinum_data = platinum_data.iloc[5:]\n",
    "    platinum_data.drop(columns=['Meta Data',0],inplace=True)\n",
    "    platinum_data.rename(columns={'index':'date'},inplace=True)\n",
    "    platinum_data.columns = [f'platinum{col[2:]}' if i >= 1 else col for i, col in enumerate(platinum_data.columns)]\n",
    "\n",
    "    platinum_data['date'] = pd.to_datetime(platinum_data['date'],format='%Y-%m-%d')\n",
    "    platinum_data['platinum open'] = pd.to_numeric(platinum_data['platinum open'], errors='coerce')\n",
    "    platinum_data['platinum high'] = pd.to_numeric(platinum_data['platinum high'], errors='coerce')\n",
    "    platinum_data['platinum low'] = pd.to_numeric(platinum_data['platinum low'], errors='coerce')\n",
    "    platinum_data['platinum close'] = pd.to_numeric(platinum_data['platinum close'], errors='coerce')\n",
    "    platinum_data['platinum volume'] = pd.to_numeric(platinum_data['platinum volume'], errors='coerce')\n",
    "    platinum_data['platinum high-low'] = platinum_data['platinum high'] - platinum_data['platinum low']\n",
    "    platinum_data = platinum_data.sort_values('date')\n",
    "\n",
    "    # create date range from 2000 until today and fill gaps\n",
    "\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d') \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    platinum_data = date_range.merge(platinum_data,how='outer',on='date') # get date range from 2000 until today and fill gaps\n",
    "    platinum_data = platinum_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    platinum_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    platinum = pd.concat([platinum_data,hist])\n",
    "\n",
    "    platinum[platinum.duplicated(subset=['date'],keep=False)] = platinum[(platinum.duplicated(subset=['date'],keep=False))&(platinum['type']=='new')]\n",
    "    platinum = platinum[platinum['date'].isna()==False]\n",
    "    platinum.drop(columns='type',inplace=True)\n",
    "\n",
    "    platinum = platinum[platinum['date']>='2000-01-01']\n",
    "    platinum = platinum[platinum['platinum open'].isna()==False]\n",
    "    platinum = platinum.sort_values('date')\n",
    "    platinum = platinum.reset_index(drop=True)\n",
    "    platinum.to_csv(path+\"/data_staging/platinum.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palladium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_palladium(path,key_AV):\n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    symbol = 'PALL'  # palladium \n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/palladium.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}'\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}&outputsize=full'\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    palladium_data = pd.DataFrame(data)\n",
    "    palladium_data = pd.concat([palladium_data['Meta Data'],palladium_data['Time Series (Daily)'].apply(lambda x: pd.Series(x))],axis=1)\n",
    "\n",
    "    palladium_data = palladium_data.reset_index()\n",
    "    palladium_data = palladium_data.iloc[5:]\n",
    "    palladium_data.drop(columns=['Meta Data',0],inplace=True)\n",
    "    palladium_data.rename(columns={'index':'date'},inplace=True)\n",
    "    palladium_data.columns = [f'palladium{col[2:]}' if i >= 1 else col for i, col in enumerate(palladium_data.columns)]\n",
    "\n",
    "    palladium_data['date'] = pd.to_datetime(palladium_data['date'],format='%Y-%m-%d')\n",
    "    palladium_data['palladium open'] = pd.to_numeric(palladium_data['palladium open'], errors='coerce')\n",
    "    palladium_data['palladium high'] = pd.to_numeric(palladium_data['palladium high'], errors='coerce')\n",
    "    palladium_data['palladium low'] = pd.to_numeric(palladium_data['palladium low'], errors='coerce')\n",
    "    palladium_data['palladium close'] = pd.to_numeric(palladium_data['palladium close'], errors='coerce')\n",
    "    palladium_data['palladium volume'] = pd.to_numeric(palladium_data['palladium volume'], errors='coerce')\n",
    "    palladium_data['palladium high-low'] = palladium_data['palladium high'] - palladium_data['palladium low']\n",
    "    palladium_data = palladium_data.sort_values('date')\n",
    "\n",
    "    # create date range from 2000 until today and fill gaps\n",
    "\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d') \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    palladium_data = date_range.merge(palladium_data,how='outer',on='date') # get date range from 2000 until today and fill gaps\n",
    "    palladium_data = palladium_data.ffill()\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    palladium_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    palladium = pd.concat([palladium_data,hist])\n",
    "\n",
    "    palladium[palladium.duplicated(subset=['date'],keep=False)] = palladium[(palladium.duplicated(subset=['date'],keep=False))&(palladium['type']=='new')]\n",
    "    palladium = palladium[palladium['date'].isna()==False]\n",
    "    palladium.drop(columns='type',inplace=True)\n",
    "\n",
    "    palladium = palladium[palladium['date']>='2000-01-01']\n",
    "    palladium = palladium[palladium['palladium open'].isna()==False]\n",
    "    palladium = palladium.sort_values('date')\n",
    "    palladium = palladium.reset_index(drop=True)\n",
    "    palladium.to_csv(path+\"/data_staging/palladium.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold(path,key_AV):\n",
    "\n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    symbol = 'GLD'  # Gold ETF\n",
    "\n",
    "    try:\n",
    "        hist = pd.read_csv(path+\"/data_staging/gold.csv\")\n",
    "        hist['date'] = pd.to_datetime(hist['date'],format='%Y-%m-%d')\n",
    "        start_date = hist['date'].iloc[-10]\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}'\n",
    "    # if the dataframe does not exist yet, we extract all the data\n",
    "    except Exception as e: \n",
    "        start_date = '2000-01-01'\n",
    "        hist = pd.DataFrame()\n",
    "        url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={key_AV}&outputsize=full'\n",
    "    end_date = datetime.now().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    gold_data = pd.DataFrame(data)\n",
    "    gold_data = pd.concat([gold_data['Meta Data'],gold_data['Time Series (Daily)'].apply(lambda x: pd.Series(x))],axis=1)\n",
    "\n",
    "    gold_data = gold_data.reset_index()\n",
    "    gold_data = gold_data.iloc[5:]\n",
    "    gold_data.drop(columns=['Meta Data',0],inplace=True)\n",
    "    gold_data.rename(columns={'index':'date'},inplace=True)\n",
    "    gold_data.columns = [f'gold{col[2:]}' if i >= 1 else col for i, col in enumerate(gold_data.columns)]\n",
    "    gold_data['date'] = pd.to_datetime(gold_data['date'],format='%Y-%m-%d')\n",
    "\n",
    "    gold_data = gold_data.sort_values('date').reset_index(drop=True)\n",
    "    # gold_data = gold_data[['date','gold open']]\n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    date_range = pd.DataFrame({'date':date_range})\n",
    "    date_range['date'] = pd.to_datetime(date_range['date'],format='%Y-%m-%d')\n",
    "\n",
    "    gold_data = date_range.merge(gold_data,how='outer',on='date')\n",
    "    gold_data = gold_data.ffill()\n",
    "    gold_data = gold_data[gold_data['gold open'].isna()==False]\n",
    "    gold_data = gold_data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    # to keep most recent available values instead of historic ones when removing duplicates\n",
    "    gold_data['type'] = 'new'\n",
    "    hist['type'] = 'history'\n",
    "\n",
    "    gold = pd.concat([gold_data,hist])\n",
    "\n",
    "    gold[gold.duplicated(subset=['date'],keep=False)] = gold[(gold.duplicated(subset=['date'],keep=False))&(gold['type']=='new')]\n",
    "    gold = gold[gold['date'].isna()==False]\n",
    "    gold.drop(columns='type',inplace=True)\n",
    "\n",
    "\n",
    "    gold = gold.sort_values(by='date')\n",
    "    gold = gold.reset_index(drop=True)\n",
    "    gold.to_csv(path+\"/data_staging/gold.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging(path):\n",
    "    \n",
    "    # API keys\n",
    "    key_FRED = 'your_FRED_key_here'\n",
    "    key_AV = 'your_AV_key_here'\n",
    "\n",
    "    try:\n",
    "        create_us_rates(path,key_AV)\n",
    "    except:\n",
    "        print('error getting rates')\n",
    "    try:\n",
    "        create_snp(path,key_AV)\n",
    "    except:\n",
    "        print('error getting snp')\n",
    "    try:\n",
    "        create_nasdaq(path,key_AV)\n",
    "    except:\n",
    "        print('error getting nasdaq')\n",
    "    try:\n",
    "        create_cpi(path,key_FRED)\n",
    "    except:\n",
    "        print('error getting cpi')\n",
    "    try:\n",
    "        create_usd_chf(path,key_FRED)\n",
    "    except:\n",
    "        print('error getting usd_chf')\n",
    "    try:\n",
    "        create_eur_usd(path,key_FRED)\n",
    "    except:\n",
    "        print('error getting eur_usd')\n",
    "    try:\n",
    "        create_gdp(path,key_FRED)\n",
    "    except:\n",
    "        print('error getting gdp')\n",
    "    try:\n",
    "        create_silver(path,key_FRED)\n",
    "    except:\n",
    "        print('error getting silver')\n",
    "    try:\n",
    "        create_oil(path,key_AV)\n",
    "    except:\n",
    "        print('error getting oil')\n",
    "    try:\n",
    "        create_platinum(path,key_AV)\n",
    "    except:\n",
    "        print('error getting platinum')\n",
    "    try:\n",
    "        create_palladium(path,key_AV)\n",
    "    except:\n",
    "        print('error getting palladium')\n",
    "    try:\n",
    "        create_gold(path,key_AV)\n",
    "    except:\n",
    "        print('error getting gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
